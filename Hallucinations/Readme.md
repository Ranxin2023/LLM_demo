# Hallucinations
## Table of Contents
- [Definition](#definition)
- [Three Main Types of Hallucinations](#three-main-types-of-hallucinations)
    - [Fact Conflicting Hallucination](#1-fact-conflicting-hallucination)
    - [Input Conflicting Hallucination](#2-input-conflicting-hallucination)
- [Why do LLMs Hallucinate](#why-do-llms-hallucinate)
- [Strategies to Reduce LLM Hallucinations](#strategies-to-reduce-llm-hallucinations)
    - [Advanced Prompting Methods](#1-advanced-prompting-methods)
    - [RAG](#2-retrieval-augmented-generation-rag)
    - [Few Shot and Zero Shot Learning](#3-few-shot-and-zero-shot-learning)
    - [Fine Tuning LLMs](#4-fine-tuning-llms)
- [Whole Picture of Hallucination](#whole-picture-of-hallucination)
    - [Picture Explanation](#picture-explanation)
    - [RAG](#rag)
        - [LLM Augmenter](#1-llm-augmenter)
        - [Fresh Prompt](#2-freshprompt)
## Definition
- An AI hallucination refers to an output generated by an AI model that **deviates from reality or lacks a factual basis**.
## Three main types of hallucinations
### 1. Fact-Conflicting Hallucination
- **Defintion**:
    - A fact-conflicting hallucination happens when the model generates information that directly contradicts known facts or truth.
    - In other words, the model produces content that sounds plausible but is actually false or inaccurate.
- **Example**:
    - If you ask an AI, “Who discovered gravity?” and it replies, “Albert Einstein discovered gravity,” that’s a fact-conflicting hallucination — because gravity was discovered by Isaac Newton.
- **Why It Happens:**
    - The model’s training data might contain **incorrect** or **conflicting information**.
    - The model may **generalize** or **infer** wrongly when trying to fill in gaps.
    - Errors can occur during **any stage of the LLM’s lifecycle** — including pre-training, fine-tuning, or inference (response generation).

### 2. Input-Conflicting Hallucination
- **Definition**:
    - This type occurs when the model’s output doesn’t align with the **user’s instructions** or input.
    - It’s a **failure to properly interpret** or follow the user’s intended task.
- **Example**:
    - If you ask the model to summarize an article about climate change, but it generates a summary about renewable energy policies not present in the article, it’s an input-conflicting hallucination.
    
### 3. Context-Conflicting Hallucination
- **Definition**:
    - A context-conflicting hallucination happens when an AI model’s output **contradicts itself or loses consistency** within a longer conversation or passage.
- **Detailed Explanation:**
    - These hallucinations occur when the LLM:
        - **Forgets earlier context** in a conversation.
        - **Contradicts something** it said previously.
        - Produces **incoherent or self-conflicting** answers.
    - This is common in **multi-turn dialogues** or long-form text generation, where maintaining coherence over many exchanges is difficult.
    
## Why do LLMs hallucinate?
### 1. Example: Google Bard’s Hallucination Case
- In 2023, Google’s LLM chatbot **Bard** gave an incorrect answer during a public demo.
- **Lesson**:This example shows that even advanced AI systems can confidently produce wrong information — a textbook case of AI hallucination.
### 2. Core Reason: Nature of LLM Training
- LLMs don’t “understand” the world or facts like humans do — they are statistical models that learn patterns in massive text data.
    - They predict **the next word** in a sequence based on probabilities learned from data.
    - They do **not have true comprehension, awareness, or factual grounding**.
    - If the data they were trained on contained **errors**, **biases**, or **outdated info**, they can reproduce or even amplify those issues.
### 3. Fluency vs. Factual Accuracy
- LLMs are optimized to sound **fluent**, **coherent**, and **natural**, not necessarily to be correct.
- That means they may:
    - Write sentences that sound **confident and grammatically perfect**,
    - But the content itself could be **completely false or made up**.
- This happens especially when:
    - The prompt is **ambiguous** (unclear question or missing context),
    - 
## How Are LLM Hallucinations Detected?
- Detecting hallucinations means checking whether the model’s output is trustworthy, factual, and consistent.
### 1. **Cross-Referencing with Trusted Sources**
- This method involves comparing the model’s generated text against reliable external databases or factual sources.
    - The goal is to see if the model’s claims can be **verified**.
    - Trusted sources include:
        - **Structured factual databases** (like Wikidata or DBpedia),
        - **Reputable news sites** (for recent events),
        - **Peer-reviewed journals** (for scientific facts).
### 2. **Out-of-Distribution (OOD) Detection**
- This approach detects hallucinations by checking when the model is **less confident** or when an input is **unfamiliar** (outside the type of data the model was trained on).
### 3. **Combining Both Approaches**
- Modern hallucination detection often combines:
    - **Automated factual validation** (checking against databases or APIs),
    - **OOD uncertainty estimation**, and
    - **Human evaluation**, especially for nuanced or context-based answers.

## Strategies to reduce LLM hallucinations
### 1. Advanced Prompting Methods
- **Definition**:
    - Advanced prompting refers to designing smarter, structured prompts that help guide the LLM’s reasoning and constrain its responses to be factual and logical.
    - It improves the model’s understanding of the task and encourages step-by-step reasoning instead of random guessing.
- **a. Chain-of-Thought Prompting (CoT)**
    - This technique instructs the model to break down its reasoning into logical steps before giving the final answer.
        - It helps the model reason more transparently, which reduces errors caused by skipping steps or jumping to conclusions.
        - Especially useful in complex reasoning, math, or explanation tasks.
    - **Example:**
        - Instead of asking:

- **b. Few-Shot Prompting**
    - Few-shot prompting involves showing the model **a few examples** of how you want it to respond **within the prompt**.
    - These examples guide the model toward the correct style, tone, and factual precision.
        - This narrows down the model’s possible outputs.
        - The model learns “what a correct answer looks like” and imitates that style.
        
### 2. Retrieval-Augmented Generation (RAG)
- **Definition**:
    - **RAG** combines **information retrieval** (from external sources) with **text generation** to ensure that the AI’s answers are grounded in real, verifiable facts.
        - It retrieves relevant documents or data from knowledge bases (like Wikipedia, PDFs, or internal databases).
        - The retrieved information is then inserted into the prompt, giving the LLM verified context before it generates the response.
- **How It Helps**:
    - Prevents the LLM from making up facts.
    - Keeps responses **contextually accurate** and up to date.
    - Reduces the chance of the model generating plausible but incorrect content.
- **RAG Benchmarks and Tools**
    - Even though RAG reduces hallucination, LLMs can still sometimes contradict retrieved data.
    - To fix this, researchers use **evaluation benchmarks** to measure how well RAG prevents hallucinations:
        - **RGB (Retrieval-Augmented Generation Benchmark)**:
            - A dataset used for testing RAG systems in English and Chinese.

#### **Step by Step of How RAG (Retrieval-Augmented Generation) helps Reduce or Eliminate Hallucinations**
- **Step1: Generate Response**
    - The **LLM first generates a draft answer** based on the user’s query.
    - This response may include product **names**, **facts**, or **entities** mentioned by the model.
    - At this stage, hallucinations might exist — the model could invent product names that **don’t actually exist** in the database.
- **Step 2: Extract Product Names from the Response**
    - The system then **extracts all entities or product names** from the generated response using **Named Entity Recognition (NER)** or similar techniques.
    - his transforms the free-form text into **structured data**, e.g., a list of product names.
    - **Example Output**:
        ```text
            ["MacBook Air 15", "MacBook Pro X Ultra 2023"]
        ```
- **Step 3: Compare with Product Name List**
    - The extracted names are **compared against a verified database or whitelist**.
    - This whitelist could come from:
        - A **company’s official product catalog**
        - A **retrieved knowledge base** (from the “Retrieval” step of RAG)
        - A **structured dataset** or **knowledge graph**
- **Step 4: Decide Whether to Reply**
    - The system then decides:
        - **If all extracted entities exist** → safely return the model’s answer.
        - **If some names are missing or fake** → discard, revise, or regenerate the response.
    - In practice, the pipeline might:
        - Re-run the generation step with corrected context
        - Ask the retriever to provide verified references
        - Replace hallucinated terms with factual data from the database
    - **Outcome**: Only responses consistent with retrieved, verified data are delivered to the user.
- **Summary: How RAG Removes Hallucination**

| **Stage**             | **Function**                  | How It Reduces Hallucination  |
| --------------------- | ----------------------------- | ----------------------------- |
| 1️.  Generate Response | LLM proposes an answer        | May contain hallucinated info |
| 2️.  Extract Entities  | Identify factual claims       | Enables verification          |
| 3️.  Verify Against DB | Cross-check with trusted data | Filters false information     |
| 4️.  Approve or Reject | Decide to show or regenerate  | Ensures factual reliability   |

### 3. Few-Shot and Zero-Shot Learning
- **Few-Shot Learning**
    - The model is given **a few examples** before performing a task.
    - These examples help the model infer **patterns**, **tone**, and **factual context**.
    - It minimizes errors that arise when the model misunderstands the desired output or context.
- **Zero-Shot Learning**
    - The model receives **no examples**, but relies on its general language knowledge.
    - This is useful when examples aren’t available or for new types of tasks.
    - Despite not being explicitly trained, zero-shot learning still allows LLMs to reason based on prior linguistic patterns — helping them avoid unsupported assumptions.

### 4. **Fine-Tuning LLMs**
- **Definition**:
    - Fine-tuning means **retraining an existing LLM** on a **smaller**, **domain-specific** dataset that contains verified, factual, and updated information.
    - This process aligns the model’s outputs with a specific **domain**, **task**, or **knowledge base**, improving accuracy and reducing outdated or incorrect claims.
- **How Fine-Tuning Reduces Hallucination:**
    - It corrects or updates the model’s internal knowledge.
    - It reinforces factual grounding and discourages speculative answers.
    - It helps the model learn contextual nuances (e.g., medical, legal, or scientific language).

## Whole Picture of Hallucination
![Hallucination](/images/hallucination_whole_picture.png)
### Picture Explanation
#### Root: Hallucination Mitigation Techniques in LLMs
- Hallucination = when LLMs generate:
    - Factually incorrect information
    - Fabricated citations
    - Confident but wrong answers
    - Logically inconsistent outputs
- The diagram shows where in the pipeline we can intervene.
#### Section 1: Prompt Engineering (§2)
- These methods do NOT retrain the model.
- They modify how we interact with the model.
- This is cheaper and easier than retraining.
#### 2.1 Retrieval-Augmented Generation (RAG)
- This branch is divided based on when retrieval happens.
#### Before Generation (§2.1.1)
- Retrieval happens BEFORE the model starts generating.
- Examples:
    - LLM-Augmenter
    - FreshPrompt
- Idea:
    - Inject retrieved facts into the prompt first.
- Flow:
    ```pgsql
    User query → Retrieve documents → Insert into prompt → Generate answer

    ```
- This grounds the model in factual data.
#### During Generation (§2.1.2)
### RAG 
#### 1. LLM-Augmenter
- **What it does**:
    - Adds small trainable modules (adapters) to LLMs.
- **Important**:
    - This is not pure vanilla RAG.
    - It blends retrieval with lightweight fine-tuning.
- **How it works**:
    - Add small adapter layers into transformer blocks
    - Inject retrieved knowledge
    - Fine-tune only small parameters
    - Instead of retraining billions of weights, you train small modules.
- Why this reduces hallucination:
    - The model becomes specialized for specific knowledge domains
    - Adapters inject structured or retrieved information
#### 2. FreshPrompt
- **Core Idea:**
    - Use a real-time search engine.
- **Problem:**
    - LLMs are trained on static data.
- **Example:**
    - GPT-3 doesn’t know events after 2021.
- **FreshPrompt Solution:**
```pgsql
User asks → System queries Bing/Google → Inject latest info → LLM answers

```
- So instead of trusting old model memory:
    - It fetches current information
    - Adds it into prompt
- **Why it helps hallucination:**
    - Because the model is no longer guessing from memory — it uses up-to-date factual sources.
- This is Before Generation RAG.
#### 3. Knowledge Retrieval
- This is the most classic RAG form.
- **Two retrieval types:**
    - (A) Keyword Search (Sparse Retrieval)
        - BM25
        - TF-IDF
    - (B) Embedding-Based Retrieval (Dense Retrieval)
        - Convert query to vector
        - Convert documents to vectors
        - Use cosine similarity
        