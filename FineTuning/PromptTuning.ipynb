{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**1. Import and Install**"
      ],
      "metadata": {
        "id": "Zu47-gc7S_Hs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U peft transformers accelerate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tr98t4J3aj5f",
        "outputId": "9ff5e3d7-561d-4010-c1fa-c1e5d8c155d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.18.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.6)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from peft) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from peft) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from peft) (6.0.3)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from peft) (2.9.0+cu126)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from peft) (4.67.1)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from peft) (0.7.0)\n",
            "Requirement already satisfied: huggingface_hub>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from peft) (0.36.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (1.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.5.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2026.1.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ucSl8rDPSeuE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import random\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Any\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    set_seed,\n",
        ")\n",
        "\n",
        "from peft import (\n",
        "    PromptTuningConfig,\n",
        "    PromptTuningInit,\n",
        "    TaskType,\n",
        "    get_peft_model,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Repro / device**"
      ],
      "metadata": {
        "id": "KgNkXD4zTXt6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(42)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
      ],
      "metadata": {
        "id": "vDvQOaWhSpTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Build a non-trivial dataset(multi-format instruction tuning)**"
      ],
      "metadata": {
        "id": "Wb5atw4bTg_l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TASKS = [\n",
        "    {\n",
        "        \"task\": \"format_to_json\",\n",
        "        \"instruction\": \"Convert the user info into strict JSON with keys: name, age, city.\",\n",
        "        \"examples\": [\n",
        "            (\"Name: Alice; Age: 24; City: Seattle\", '{\"name\":\"Alice\",\"age\":24,\"city\":\"Seattle\"}'),\n",
        "            (\"Name: Bob; Age: 31; City: Austin\", '{\"name\":\"Bob\",\"age\":31,\"city\":\"Austin\"}'),\n",
        "        ],\n",
        "    },\n",
        "    {\n",
        "        \"task\": \"classify_sentiment\",\n",
        "        \"instruction\": \"Classify sentiment as one of: positive, negative, neutral. Output only the label.\",\n",
        "        \"examples\": [\n",
        "            (\"I love this phone, battery lasts forever.\", \"positive\"),\n",
        "            (\"This update broke everything and I’m furious.\", \"negative\"),\n",
        "            (\"It arrived yesterday.\", \"neutral\"),\n",
        "        ],\n",
        "    },\n",
        "    {\n",
        "        \"task\": \"extract_fields\",\n",
        "        \"instruction\": \"Extract email fields. Output as: TO=<...> | SUBJECT=<...> | ACTION=<...>.\",\n",
        "        \"examples\": [\n",
        "            (\"Email: To John, Subject: Meeting, Please reschedule to Friday.\", \"TO=John | SUBJECT=Meeting | ACTION=reschedule to Friday\"),\n",
        "            (\"Email: To HR, Subject: PTO, I want to request 2 days off next week.\", \"TO=HR | SUBJECT=PTO | ACTION=request 2 days off next week\"),\n",
        "        ],\n",
        "    },\n",
        "]"
      ],
      "metadata": {
        "id": "O5xkiEm2TpRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_synthetic_samples(n: int = 300) -> List[Dict[str, str]]:\n",
        "    \"\"\"Create a mixed-task dataset so the prompt vector learns broader steering.\"\"\"\n",
        "    samples = []\n",
        "    for _ in range(n):\n",
        "        t = random.choice(TASKS)\n",
        "        inp, out = random.choice(t[\"examples\"])\n",
        "\n",
        "        # Add small perturbations to make it harder than pure memorization\n",
        "        if t[\"task\"] == \"format_to_json\" and random.random() < 0.3:\n",
        "            inp = inp.replace(\";\", \",\").replace(\"Age:\", \"age:\")\n",
        "        if t[\"task\"] == \"classify_sentiment\" and random.random() < 0.3:\n",
        "            inp = inp + \" \" + random.choice([\"Honestly.\", \"FYI.\", \"No joke.\"])\n",
        "        if t[\"task\"] == \"extract_fields\" and random.random() < 0.3:\n",
        "            inp = inp.replace(\"Email:\", \"Message:\")\n",
        "\n",
        "        samples.append({\n",
        "            \"task\": t[\"task\"],\n",
        "            \"instruction\": t[\"instruction\"],\n",
        "            \"input\": inp,\n",
        "            \"output\": out,\n",
        "        })\n",
        "    return samples"
      ],
      "metadata": {
        "id": "GnRf56sLTtUR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ALL = make_synthetic_samples(1000)\n",
        "random.shuffle(ALL)\n",
        "split = int(0.9 * len(ALL))\n",
        "TRAIN = ALL[:split]\n",
        "EVAL = ALL[split:]\n"
      ],
      "metadata": {
        "id": "sNmx2BJrTwoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Tokenizer + formatting**"
      ],
      "metadata": {
        "id": "X3KKXROyT3vQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"microsoft/phi-2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# GPT-2 has no pad_token by default; set it safely\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QlHWY-W-T8qV",
        "outputId": "17acc28b-04d1-4d6a-a30b-08df603a8d8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def format_example(ex: Dict[str, str]) -> str:\n",
        "    \"\"\"\n",
        "    Instruction-style prompt that the tuned virtual tokens will \"steer\".\n",
        "    We keep a consistent template (important for soft prompt learning).\n",
        "    \"\"\"\n",
        "    return (\n",
        "        \"### System\\n\"\n",
        "        \"You are a precise assistant. Follow the user instruction exactly.\\n\\n\"\n",
        "        \"### Instruction\\n\"\n",
        "        f\"{ex['instruction']}\\n\\n\"\n",
        "        \"### Input\\n\"\n",
        "        f\"{ex['input']}\\n\\n\"\n",
        "        \"### Output\\n\"\n",
        "        f\"{ex['output']}\"\n",
        "    )"
      ],
      "metadata": {
        "id": "_2pNwsCQUG-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_prompt_for_inference(instruction: str, user_input: str) -> str:\n",
        "    \"\"\"Same template but WITHOUT the ground-truth output.\"\"\"\n",
        "    return (\n",
        "        \"### System\\n\"\n",
        "        \"You are a precise assistant. Follow the user instruction exactly.\\n\\n\"\n",
        "        \"### Instruction\\n\"\n",
        "        f\"{instruction}\\n\\n\"\n",
        "        \"### Input\\n\"\n",
        "        f\"{user_input}\\n\\n\"\n",
        "        \"### Output\\n\"\n",
        "    )"
      ],
      "metadata": {
        "id": "yVdbDA3SUIJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MixedInstructionDataset(Dataset):\n",
        "    def __init__(self, rows: List[Dict[str, str]], max_length: int = 256):\n",
        "        self.rows = rows\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.rows)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
        "      ex = self.rows[idx]\n",
        "\n",
        "      # Build prompt WITHOUT the ground-truth answer\n",
        "      prompt = (\n",
        "          \"### System\\n\"\n",
        "          \"You are a precise assistant. Follow the user instruction exactly.\\n\\n\"\n",
        "          \"### Instruction\\n\"\n",
        "          f\"{ex['instruction']}\\n\\n\"\n",
        "          \"### Input\\n\"\n",
        "          f\"{ex['input']}\\n\\n\"\n",
        "          \"### Response\\n\"\n",
        "      )\n",
        "\n",
        "      # Ground-truth answer (keep it clean, no extra headers)\n",
        "      answer = ex[\"output\"].strip()\n",
        "\n",
        "      # Tokenize prompt and full text separately\n",
        "      prompt_enc = tokenizer(\n",
        "          prompt,\n",
        "          truncation=True,\n",
        "          max_length=self.max_length,\n",
        "          padding=False,\n",
        "          return_tensors=\"pt\",\n",
        "          add_special_tokens=False,\n",
        "      )\n",
        "      full_enc = tokenizer(\n",
        "          prompt + answer,\n",
        "          truncation=True,\n",
        "          max_length=self.max_length,\n",
        "          padding=False,\n",
        "          return_tensors=\"pt\",\n",
        "          add_special_tokens=False,\n",
        "      )\n",
        "\n",
        "      input_ids = full_enc[\"input_ids\"].squeeze(0)\n",
        "      attention_mask = full_enc[\"attention_mask\"].squeeze(0)\n",
        "\n",
        "      # Labels: mask out prompt tokens so loss only applies to answer\n",
        "      labels = input_ids.clone()\n",
        "      prompt_len = prompt_enc[\"input_ids\"].size(1)\n",
        "      labels[:prompt_len] = -100\n",
        "\n",
        "      return {\n",
        "          \"input_ids\": input_ids,\n",
        "          \"attention_mask\": attention_mask,\n",
        "          \"labels\": labels,\n",
        "      }\n"
      ],
      "metadata": {
        "id": "xrPev-2eUKhh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class CausalLMCollator:\n",
        "    pad_token_id: int\n",
        "    num_virtual_tokens: int\n",
        "    label_pad_id: int = -100\n",
        "\n",
        "    def __call__(self, features):\n",
        "        input_ids = [f[\"input_ids\"] for f in features]\n",
        "        attention_mask = [f[\"attention_mask\"] for f in features]\n",
        "        labels = [f[\"labels\"] for f in features]\n",
        "\n",
        "        # IMPORTANT: account for virtual tokens added by prompt tuning\n",
        "        if self.num_virtual_tokens > 0:\n",
        "            vt = self.num_virtual_tokens\n",
        "            labels = [\n",
        "                torch.cat([torch.full((vt,), self.label_pad_id, dtype=l.dtype), l], dim=0)\n",
        "                for l in labels\n",
        "            ]\n",
        "            attention_mask = [\n",
        "                torch.cat([torch.ones(vt, dtype=a.dtype), a], dim=0)\n",
        "                for a in attention_mask\n",
        "            ]\n",
        "\n",
        "        max_len = max(x.size(0) for x in labels)  # labels now define the true length\n",
        "\n",
        "        def pad_1d(x, pad_value):\n",
        "            pad_len = max_len - x.size(0)\n",
        "            if pad_len <= 0:\n",
        "                return x\n",
        "            return torch.cat([x, torch.full((pad_len,), pad_value, dtype=x.dtype)], dim=0)\n",
        "\n",
        "        # input_ids SHOULD remain the original length; we pad it to match max_len by left-padding?\n",
        "        # For GPT-2, safest is RIGHT-pad, but we must align with labels/attention_mask length.\n",
        "        input_ids = torch.stack([pad_1d(x, self.pad_token_id) for x in input_ids])\n",
        "        attention_mask = torch.stack([pad_1d(x, 0) for x in attention_mask])\n",
        "        labels = torch.stack([pad_1d(x, self.label_pad_id) for x in labels])\n",
        "\n",
        "        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n",
        "\n"
      ],
      "metadata": {
        "id": "QbqHHgrsYqws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5.  Base model + Prompt Tuning (PEFT)**"
      ],
      "metadata": {
        "id": "rBNyfyyvY50z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# -----------------------------\n",
        "# Base model (phi-2)\n",
        "# -----------------------------\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    trust_remote_code=True,        # REQUIRED for phi-2\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Make sure pad token is defined at model level\n",
        "base_model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "# -----------------------------\n",
        "# Prompt tuning config (FIXED)\n",
        "# -----------------------------\n",
        "peft_config = PromptTuningConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "\n",
        "    # TEXT init is good, but keep it SHORT\n",
        "    prompt_tuning_init=PromptTuningInit.TEXT,\n",
        "    prompt_tuning_init_text=(\n",
        "        \"Follow the instruction. \"\n",
        "        \"Output only the final answer.\"\n",
        "    ),\n",
        "\n",
        "    # IMPORTANT: reduce this for phi-2\n",
        "    num_virtual_tokens=8,   # ← 8 or 16 max for phi-2\n",
        "\n",
        "    tokenizer_name_or_path=model_name,\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# Apply PEFT\n",
        "# -----------------------------\n",
        "model = get_peft_model(base_model, peft_config)\n",
        "\n",
        "# Sanity check: trainable params should be tiny\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 674,
          "referenced_widgets": [
            "ea5386ca20e74fd7a2ec5da9f66c4f68",
            "eec880a5bc454ea6869dec588e84775e",
            "65286ba37b304eff8c3fe7d6dd0887c1",
            "095e7324e31e40ecb9d043cae5966b7e",
            "99f9ae98a0df4a46ae34176400b4023f",
            "e65fed431ada4ed9ae622b221ea0677c",
            "142a2515cfc44e1e980ee51f61b1eb48",
            "93b77c1a30e9415d90e9e75e34a5cd49",
            "485d7af407bd4708a1e43aba340500d9",
            "57493a9803ba4636b8627f069f44a355",
            "f77dc880e1df4cf8bcc6ea1964bf32b0"
          ]
        },
        "id": "hrN_uCxVY9l7",
        "outputId": "81e5d8ec-ea31-4d1a-db96-17f958a107e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ea5386ca20e74fd7a2ec5da9f66c4f68"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 20,480 || all params: 2,779,704,320 || trainable%: 0.0007\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): PhiForCausalLM(\n",
              "    (model): PhiModel(\n",
              "      (embed_tokens): Embedding(51200, 2560)\n",
              "      (layers): ModuleList(\n",
              "        (0-31): 32 x PhiDecoderLayer(\n",
              "          (self_attn): PhiAttention(\n",
              "            (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
              "            (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
              "            (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
              "            (dense): Linear(in_features=2560, out_features=2560, bias=True)\n",
              "          )\n",
              "          (mlp): PhiMLP(\n",
              "            (activation_fn): NewGELUActivation()\n",
              "            (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
              "            (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
              "          )\n",
              "          (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (rotary_emb): PhiRotaryEmbedding()\n",
              "      (embed_dropout): Dropout(p=0.0, inplace=False)\n",
              "      (final_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (lm_head): Linear(in_features=2560, out_features=51200, bias=True)\n",
              "  )\n",
              "  (prompt_encoder): ModuleDict(\n",
              "    (default): PromptEmbedding(\n",
              "      (embedding): Embedding(8, 2560)\n",
              "    )\n",
              "  )\n",
              "  (word_embeddings): Embedding(51200, 2560)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_ds = MixedInstructionDataset(TRAIN, max_length=256)\n",
        "eval_ds  = MixedInstructionDataset(EVAL,  max_length=256)\n",
        "collator = CausalLMCollator(\n",
        "    pad_token_id=tokenizer.pad_token_id,\n",
        "    num_virtual_tokens=peft_config.num_virtual_tokens\n",
        ")\n"
      ],
      "metadata": {
        "id": "yQwFqYitYyxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6.  Metrics (optional): perplexity on eval**"
      ],
      "metadata": {
        "id": "1a5-myCQZSct"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    # Trainer gives (logits, labels) but labels contain -100 pads\n",
        "    logits, labels = eval_pred\n",
        "    # We compute a rough perplexity from cross-entropy ignoring -100 tokens.\n",
        "    logits_t = torch.tensor(logits)\n",
        "    labels_t = torch.tensor(labels)\n",
        "\n",
        "    # shift for causal LM\n",
        "    shift_logits = logits_t[:, :-1, :].contiguous()\n",
        "    shift_labels = labels_t[:, 1:].contiguous()\n",
        "\n",
        "    loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
        "    loss = loss_fct(\n",
        "        shift_logits.view(-1, shift_logits.size(-1)),\n",
        "        shift_labels.view(-1),\n",
        "    )\n",
        "    ppl = float(torch.exp(loss).clamp(max=1e4))\n",
        "    return {\"eval_loss_ce\": float(loss), \"perplexity\": ppl}"
      ],
      "metadata": {
        "id": "cSBl6B5bZVKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7. Train**"
      ],
      "metadata": {
        "id": "bywwFl_OZbE8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class PromptTuningTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
        "        # Forward pass (DO NOT pass labels to model to avoid default loss)\n",
        "        labels = inputs.pop(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits  # [B, S_logit, V]\n",
        "\n",
        "        # Align labels length with logits length (pad LEFT with -100)\n",
        "        if logits.size(1) != labels.size(1):\n",
        "            diff = logits.size(1) - labels.size(1)\n",
        "            if diff > 0:\n",
        "                pad = torch.full((labels.size(0), diff), -100, device=labels.device, dtype=labels.dtype)\n",
        "                labels = torch.cat([pad, labels], dim=1)\n",
        "            else:\n",
        "                labels = labels[:, -logits.size(1):]\n",
        "\n",
        "        # Shift for causal LM\n",
        "        shift_logits = logits[:, :-1, :].contiguous()\n",
        "        shift_labels = labels[:, 1:].contiguous()\n",
        "\n",
        "        loss = F.cross_entropy(\n",
        "            shift_logits.view(-1, shift_logits.size(-1)),\n",
        "            shift_labels.view(-1),\n",
        "            ignore_index=-100\n",
        "        )\n",
        "\n",
        "        return (loss, outputs) if return_outputs else loss"
      ],
      "metadata": {
        "id": "3GjhlxVIcE8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out_dir = \"./prompt_tuned_gpt2\"\n",
        "args = TrainingArguments(\n",
        "    output_dir=out_dir,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    gradient_accumulation_steps=2,\n",
        "    learning_rate=2e-3,           # prompt vectors can handle higher LR\n",
        "    num_train_epochs=60,\n",
        "    warmup_ratio=0.06,\n",
        "    weight_decay=0.0,\n",
        "    logging_steps=20,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=2,\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    report_to=\"none\",\n",
        ")"
      ],
      "metadata": {
        "id": "zqLEhwY2ZdFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = PromptTuningTrainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=eval_ds,\n",
        "    data_collator=collator,\n",
        ")\n",
        "trainer.train()\n",
        "trainer.evaluate()\n",
        "\n",
        "# Save ONLY the prompt-tuning adapter (small)\n",
        "model.save_pretrained(out_dir)\n",
        "tokenizer.save_pretrained(out_dir)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Nkb05NEvZmnU",
        "outputId": "1a91ab03-ded9-40a0-d09d-a1ddff4e61b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3420' max='3420' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3420/3420 1:30:54, Epoch 60/60]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>27.218300</td>\n",
              "      <td>11.586146</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>19.566700</td>\n",
              "      <td>8.108616</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>14.231400</td>\n",
              "      <td>5.678496</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>9.771700</td>\n",
              "      <td>3.784669</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>5.645200</td>\n",
              "      <td>1.697176</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>3.312100</td>\n",
              "      <td>1.153710</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>2.686600</td>\n",
              "      <td>0.793910</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>2.239100</td>\n",
              "      <td>0.821090</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.794100</td>\n",
              "      <td>0.597276</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.699800</td>\n",
              "      <td>0.457002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>1.364600</td>\n",
              "      <td>0.417067</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>1.196200</td>\n",
              "      <td>0.430622</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>1.022700</td>\n",
              "      <td>0.580184</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.950200</td>\n",
              "      <td>0.181863</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.842400</td>\n",
              "      <td>0.128389</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.797200</td>\n",
              "      <td>0.226975</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.712900</td>\n",
              "      <td>0.162930</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.747800</td>\n",
              "      <td>0.083777</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.630300</td>\n",
              "      <td>0.071610</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.513800</td>\n",
              "      <td>0.140142</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.595400</td>\n",
              "      <td>0.034180</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>0.550400</td>\n",
              "      <td>0.506659</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.551900</td>\n",
              "      <td>0.039762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>0.434600</td>\n",
              "      <td>0.025383</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.411400</td>\n",
              "      <td>0.165341</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>0.517600</td>\n",
              "      <td>0.031441</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>0.388500</td>\n",
              "      <td>0.085779</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.359100</td>\n",
              "      <td>0.164785</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>0.396100</td>\n",
              "      <td>0.099887</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.349100</td>\n",
              "      <td>0.041667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>0.489300</td>\n",
              "      <td>0.029080</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>0.301500</td>\n",
              "      <td>0.049723</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>0.271400</td>\n",
              "      <td>0.116770</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>0.337800</td>\n",
              "      <td>0.017926</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.345100</td>\n",
              "      <td>0.006756</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>0.259100</td>\n",
              "      <td>0.015686</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>0.261100</td>\n",
              "      <td>0.113455</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>0.220400</td>\n",
              "      <td>0.014714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>0.273300</td>\n",
              "      <td>0.111377</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.227700</td>\n",
              "      <td>0.079873</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>0.214900</td>\n",
              "      <td>0.095853</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>0.220500</td>\n",
              "      <td>0.013237</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>0.253900</td>\n",
              "      <td>0.040348</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>0.220000</td>\n",
              "      <td>0.125057</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>0.164900</td>\n",
              "      <td>0.072453</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>0.189700</td>\n",
              "      <td>0.131009</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>0.184700</td>\n",
              "      <td>0.004111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>0.181200</td>\n",
              "      <td>0.017236</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>0.152300</td>\n",
              "      <td>0.018490</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.160900</td>\n",
              "      <td>0.003037</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>0.165200</td>\n",
              "      <td>0.002821</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>0.168100</td>\n",
              "      <td>0.021351</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>0.149200</td>\n",
              "      <td>0.023593</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>0.127400</td>\n",
              "      <td>0.063823</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>0.137400</td>\n",
              "      <td>0.026233</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>0.138400</td>\n",
              "      <td>0.004435</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>0.146100</td>\n",
              "      <td>0.013819</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>0.122700</td>\n",
              "      <td>0.038713</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>0.132400</td>\n",
              "      <td>0.027554</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.112500</td>\n",
              "      <td>0.041030</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [13/13 00:04]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./prompt_tuned_gpt2/tokenizer_config.json',\n",
              " './prompt_tuned_gpt2/special_tokens_map.json',\n",
              " './prompt_tuned_gpt2/vocab.json',\n",
              " './prompt_tuned_gpt2/merges.txt',\n",
              " './prompt_tuned_gpt2/added_tokens.json',\n",
              " './prompt_tuned_gpt2/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8. Inference helper**"
      ],
      "metadata": {
        "id": "izvHYnUakGEn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def generate_answer(instruction: str, user_input: str, max_new_tokens: int = 60) -> str:\n",
        "    model.eval()\n",
        "    prompt = format_prompt_for_inference(instruction, user_input)\n",
        "\n",
        "    enc = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    out = model.generate(\n",
        "      **enc,\n",
        "      max_new_tokens=60,\n",
        "      do_sample=True,\n",
        "      temperature=0.7,\n",
        "      top_p=0.9,\n",
        "      repetition_penalty=1.2,\n",
        "      no_repeat_ngram_size=3,\n",
        "      eos_token_id=tokenizer.eos_token_id,\n",
        "      pad_token_id=tokenizer.eos_token_id,\n",
        "  )\n",
        "\n",
        "    text = tokenizer.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "    # Return only the generated part after \"### Output\"\n",
        "    marker = \"### Output\\n\"\n",
        "    if marker in text:\n",
        "        return text.split(marker, 1)[1].strip()\n",
        "    return text.strip()\n",
        "\n"
      ],
      "metadata": {
        "id": "Arq61P5VkJ7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9. Quick demo tests**"
      ],
      "metadata": {
        "id": "rw1YtZNQkNMc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tests = [\n",
        "    (\"Convert the user info into strict JSON with keys: name, age, city.\",\n",
        "     \"Name: Chen; Age: 29; City: Boston\"),\n",
        "    (\"Classify sentiment as one of: positive, negative, neutral. Output only the label.\",\n",
        "     \"This is fine. Nothing special.\"),\n",
        "    (\"Extract email fields. Output as: TO=<...> | SUBJECT=<...> | ACTION=<...>.\",\n",
        "     \"Email: To Alex, Subject: Budget, please reduce the cost by 10%.\"),\n",
        "]\n",
        "\n",
        "for inst, inp in tests:\n",
        "    ans = generate_answer(inst, inp)\n",
        "    print(\"\\nINSTRUCTION:\", inst)\n",
        "    print(\"INPUT:\", inp)\n",
        "    print(\"MODEL OUTPUT:\", ans)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ey_7QwWkSOC",
        "outputId": "d7f95b7d-4bf3-402d-b847-38a8a0aca479"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/peft/peft_model.py:2141: UserWarning: Position ids are not supported for parameter efficient tuning. Ignoring position ids.\n",
            "  warnings.warn(\"Position ids are not supported for parameter efficient tuning. Ignoring position ids.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "INSTRUCTION: Convert the user info into strict JSON with keys: name, age, city.\n",
            "INPUT: Name: Chen; Age: 29; City: Boston\n",
            "MODEL OUTPUT: Chen: 494958c Chen;age: 5844584c Chen.; Chen: 492958a Chen;agge: 5844465a Chen.;\n",
            "Chens: 593845c 5938458c 5938546c 59386452c\n",
            "(this is the same\n",
            "\n",
            "INSTRUCTION: Classify sentiment as one of: positive, negative, neutral. Output only the label.\n",
            "INPUT: This is fine. Nothing special.\n",
            "MODEL OUTPUT: Week 1:negative50week1negative50weeks1negative52week2negative49week3negative47week4negative45week5negative44week6positive54week7positive56week8positive58week9positive59week10positive61week11positive62week12positive63week13\n",
            "\n",
            "INSTRUCTION: Extract email fields. Output as: TO=<...> | SUBJECT=<...> | ACTION=<...>.\n",
            "INPUT: Email: To Alex, Subject: Budget, please reduce the cost by 10%.\n",
            "MODEL OUTPUT: Email:\\TOAlex@Alex. Email: Alex, Alex.JECTJECTEDalex Alex. 100=100. 200=150. 300=200. 400=250. 500=300. 600=400. 700=500. 800=600. 900=700. 1000=800.\n"
          ]
        }
      ]
    }
  ]
}